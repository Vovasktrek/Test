# DWH Prototype – COVID-19 Analytics

## 1. Источник данных

Источник: Our World in Data  
Ссылка: https://ourworldindata.org/covid-data  
CSV: https://covid.ourworldindata.org/data/owid-covid-data.csv  

Почему выбран:

- Бесплатный открытый источник
- Полная историческая статистика
- Подходит для построения витрин и BI

Основные поля:

- location (text)
- date (date)
- total_cases (numeric)
- new_cases (numeric)
- total_deaths (numeric)
- new_deaths (numeric)
- total_vaccinations (numeric)
- population (numeric)

---

## 2. Архитектура

API → Airflow → PostgreSQL (raw schema) → DWH → BI

---

## 3. Запуск проекта

1. docker-compose up -d
2. Airflow: http://localhost:8080
3. PostgreSQL: localhost:5432

## 4. Витрина данных (Data Mart)

### Выбранные метрики

1. Средняя цена закрытия (avg_close_price)
   Позволяет анализировать среднюю стоимость акции за период.

2. Дневная волатильность (volatility)
   Показывает уровень колебаний цены.

3. Объем торгов (total_volume)
   Позволяет оценить активность рынка.

### Структура витрины

Таблица: mart_stock_metrics

- trade_date
- ticker
- avg_close_price
- volatility
- total_volume

## Дашборд

BI-система: Yandex DataLens

Ссылка на опубликованный дашборд:
https://datalens.yandex/your_dashboard_link

Скриншоты находятся в папке /screenshots


"1. Какие компоненты заменил бы я

- Источник данных и хранение
- Вместо одиночной реляционной БД — распределённое хранилище: HDFS или объектное хранилище (S3/MinIO) для сырых данных.
- Для аналитических таблиц — распределённая колоночная СУБД или движок OLAP: ClickHouse, Apache Pinot, Amazon Redshift, Snowflake.

- Пайплайн обработки
- Batch-обработка: заменить одиночные ETL-скрипты на Apache Spark или Flink (для больших объёмов и распределённой обработки).
- Для стриминга событий — Kafka + Kafka Streams / Flink / Pulsar.

- Оркестрация и управление метаданными
- Airflow / Prefect для оркестрации задач; Apache Hive Metastore или Glue Data Catalog для схем и метаданных.

- Кэширование и быстрые запросы
- Добавить слой кэша/OLAP-индекса: Druid, ClickHouse, ElasticSearch (для полнотекстовых запросов), Redis для горячих агрегатов.

- Мониторинг и логирование
- Prometheus + Grafana, ELK (Elasticsearch, Logstash, Kibana) или OpenTelemetry.

- Управление схемой и качеством данных
- Deequ/Great Expectations для проверки качества, Delta Lake / Apache Hudi / Iceberg для ACID на партиционированных данных и time-travel.

2. Проблемы с производительностью в текущем пайплайне

- Узкие места дискового ввода-вывода
- Чтение/запись больших объёмов в одной БД приведёт к I/O-узким местам и блокировкам.

- Одноузловые компоненты не масштабируются
- Если ETL выполняется на одном сервере — время обработки вырастет ~в 100×.

- Сетевые и сериализационные расходы
- При распределённой обработке без оптимизации форматов (Parquet/ORC) трафик и десериализация займут много времени.

- Неоптимальные форматы хранения
- Строковые форматы (CSV/JSON) приведут к большим затратам на парсинг и хранение.

- Отсутствие партиционирования/индексации
- Полные сканирования таблиц станут дорогостоящими; увеличится latency запросов.

- Точки согласованности и блокировки транзакций
- Традиционные транзакционные СУБД упрутся в конкуренцию и локации при массовых вставках/агрегациях.

- Оркестрация и задержки
- Увеличение времени выполнения DAG-ов, задержки при зависимостях, проблемы с повторными запусками и откатом.

3. Какой движок СУБД лучше — колоночный или строковый

- Выбор зависит от метрик и типов нагрузок
- Если основная нагрузка аналитическая (агрегаты, сканирование больших объёмов, OLAP-запросы по меньшему набору олонок) — колоночный движок предпочтительнее.
- Причины: лучшая компрессия, чтение только нужных колонок, векторные операции, высокая скорость агрегаций. Примеры: ClickHouse, Amazon Redshift, Snowflake, Apache Parquet+Spark.

- Если нагрузка OLTP (много мелких транзакций, точечные чтения/записи) — строковый движок лучше.
- Причины: эффективные записи и чтение целых строк, низкая задержка для транзакций. Примеры: PostgreSQL, MySQL, distributed OLTP (CockroachDB, Yugabyte).

- Для нашей задачи при росте данных в 100×
- Предполагая аналитические метрики (сканирования, агрегаты, исторические срезы) — использовать колоночный движок.
- Комбинированный подход: хранить сырые события в объектном хранилище/партиционированных файлах (Parquet/ORC), служебные агрегаты и быстрые аналитические запросы — в колоночной СУБД; горячие точки доступа и служебные транзакции — в распределённом строковом хранилище.

- Дополнительные соображения
- Форматы хранения: Parquet/ORC для столбцовых файлов; использовать компактификацию и партиционирование по времени/ключам.
- Индексация и материалы view: предагрегированные таблицы/MV, rollup-таблицы, OLAP-кубы.
- Баланс стоимости/latency: облачные аналитические движки дают масштабирование без управления железом, но могут быть дороже"
